{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pakete installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install gymnasium\n",
    "! pip install gymnasium[classic-control]\n",
    "! pip install -U tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einen Einblick bekommen\n",
    "\n",
    "Um da Problem besser zu verstehen, ist es eine gute Idee zufällige Aktionen zu wählen und visuell zu inspizieren was passiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, info = env.reset()  # Start Simulation\n",
    "\n",
    "for _ in range(500):\n",
    "    # TODO\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()  # start the rendering for us to see"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Außerdem schauen wir uns an, wie viele verschiedene Aktionen es gibt und wie die Beobachtungen aussehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space Discrete(2)\n",
      "Action space size: 2\n",
      "Observation space shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "GAME = \"CartPole-v1\"\n",
    "env = gym.make(GAME)\n",
    "\n",
    "print(f'Action space {env.action_space}')\n",
    "print(f'Action space size: {env.action_space.n}')\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "print(f'Observation space shape: {obs[0].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen wir haben 2 diskrete Aktionen (nach links, nach rechts) aus denen wir wählen können. Unser Agent erhält vier Observationen. Der [Dokumentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/) zu Folge sind das *Position des Karts*, *Geschwindigkeit des Karts*, *Winkel der Stange* und *Winkelgeschwindigkeit der Stange*.\n",
    "\n",
    "Jetzt haben wir alle Informationen, die wir brauchen also...\n",
    "\n",
    "# Let's Start building our agent\n",
    "\n",
    "## Replay Buffer\n",
    "Wir starten in dem wir eine Klasse für den Replay Buffer definieren. Dieser speichert vorherige Zustandsübergänge und erlaubt es uns aus allen Übergängen in einem durch `buffer_size` vordefinierten Zeitfenster zufällig zu samplen. Für jeden Übergang speichern wir die vorherige Observation, die Aktion, die wir gewählt haben, den Reward den wir dafür bekommen haben, die Observation des nächsten Zustands in den wir durch Wahl der Aktion gekommen sind und die Information, ob die Simulation nach unserer Aktion beendet war."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random as rand\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size <= len(self.memory):\n",
    "            return rand.sample(self.memory, batch_size)\n",
    "        else:\n",
    "            assert False\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network class\n",
    "Wir verwenden ein relativ simples fully-connected Neuronales Netz. Unser Netz hat eine frei wählbare Anzahl an hidden layern mit ReLU-nicht-Linearität und 64 Neuronen. Das Netz hat außerdem einen Ausgabe-Layer ohne nicht-Linearität und genau so vielen Ausgabeneuronen wie er wählbare Aktionen gibt.\n",
    "\n",
    "Wir definieren außerdem eine `act` Methode, die es uns erlaubt direkt die Aktion zu erhalten, die den hächsten vorhergesahgten Q-Wert hatte.\n",
    "\n",
    "Außerdem definieren wir eine `get_model` Methode die es uns erlaubt schnell ein bereits kompiliertes Modell zu erhalten. Zudem haben wir eine `get_copy` und eine `transfer_weights` Methode, die es vereinfachen ein Target Netz zu erzeugen und zu aktualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class DQN(keras.Model):\n",
    "\n",
    "    def __init__(self, obs_space, num_actions, num_hidden_layers, optimizer, lr, loss):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.obs_space = obs_space\n",
    "        self.optimizer = optimizer\n",
    "        self.rate = lr\n",
    "        self.loss = loss\n",
    "        self.num_actions = num_actions\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # TODO: Define layers\n",
    "\n",
    "        self.input_layer = keras.layers.InputLayer(input_shape=obs_space)\n",
    "        self.hidden_layers = []\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.hidden_layers.append(keras.layers.Dense(64, activation='relu'))\n",
    "        self.output_layer = keras.layers.Dense(num_actions, activation='linear')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO: Apply layers to input\n",
    "        x = self.input_layer(inputs)\n",
    "        for l in self.hidden_layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        q_vals = self.output_layer(x)\n",
    "        \n",
    "        return q_vals\n",
    "\n",
    "    def act(self, state):\n",
    "        q_vals = self(state)\n",
    "        action = np.argmax(q_vals)\n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model(input_shape, num_action, num_hidden_layers, optimizer=keras.optimizers.Adam, lr=1e-5, loss=keras.losses.MeanSquaredError):\n",
    "        model = DQN(input_shape, num_action, num_hidden_layers, optimizer, lr, loss)\n",
    "        optimizer = deepcopy(optimizer)\n",
    "        optimizer = optimizer(learning_rate=lr)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=keras.losses.MeanSquaredError()\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'obs_space': self.obs_space,\n",
    "            'num_actions': self.num_actions,\n",
    "            'num_hidden_layers': self.num_hidden_layers,\n",
    "            'optimizer': self.optimizer,\n",
    "            'lr': self.rate,\n",
    "            'loss': self.loss\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def get_copy(self):\n",
    "        model_copy = keras.models.clone_model(self)\n",
    "        model_copy.set_weights(self.get_weights())\n",
    "        model_copy.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=keras.losses.MeanSquaredError()\n",
    "        )\n",
    "        return model_copy\n",
    "    \n",
    "    def transfer_weights(self, target_net):\n",
    "        target_net.set_weights(self.get_weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "Wir müssen einige Hyperparameter definieren. Diese Werte sind frei wählbar aber manche, wie `GAMMA = 0.99` sind de-facto Standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99                # weight of old (~1) vs new (~0) situations\n",
    "BATCH_SIZE = 32             # we know this one\n",
    "BUFFER_SIZE = 50000         # how large is my memory\n",
    "MIN_REPLAY_SIZE = 1000      # how much must I have seen before I start training\n",
    "EPSILON_START = 1.0         # in the beginning we do everything random\n",
    "EPSILON_END = 0.01          # in the end we do 1% of actions randomly\n",
    "EPSILON_DECAY = 100000      # we go from random to non-random over 100000 steps\n",
    "TARGET_UPDATE_FREQ = 1000   # every 1000 steps we update the target net\n",
    "TRAINING_FREQ = 1           # every steps we train our model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weitere Vorraussetzungen für das Training\n",
    "Bevor wir anfangen können zu trainieren müssen wir Replay und Reward Buffer initialisieren, die Netzwerke erzeugen und den Replay Buffer soweit befüllen, dass wir sinnvoll aus diesem samplen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "reward_buffer = deque([0.0], maxlen = 100)\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_net = DQN.get_model(obs[0].shape, env.action_space.n, 1, keras.optimizers.Adam, 1e-4)\n",
    "target_net = online_net.get_copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auffüllen des Replay Buffers, so dass wir genug Daten haben, um mit dem Training zu beginnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_env(action, last_obs, is_training=True):\n",
    "    global episode_reward, reward_buffer\n",
    "    \n",
    "    # TODO: Step env\n",
    "    new_obs, reward, done, truncated, _ = env.step(action)\n",
    "    replay_buffer.add(last_obs, action, reward, new_obs, done)\n",
    "    obs = new_obs\n",
    "\n",
    "    if is_training:\n",
    "        episode_reward += reward\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "        if is_training:\n",
    "            reward_buffer.append(episode_reward)\n",
    "            episode_reward = 0.\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    # TODO: Choose random action\n",
    "    action = env.action_space.sample()\n",
    "    obs = step_env(action, obs, is_training=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-greedy Policy\n",
    "Die Epsilon-greedy Policy besagt, dass wir eine zufällige Aktion mit Wahrscheinlichkeit Epsilon wählen und mit Wahrscheinlichkeit 1-Epsilon das gelernte Wissen unseres Netzes ausnutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_epsilon_greedy_action(obs, epsilon):\n",
    "    # TODO: Implement epsilon-greedy policy\n",
    "    rand_num = random.random()\n",
    "    if rand_num <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = online_net.act(np.exand_dims(obs, 0))\n",
    "\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Für das Training samplen wir zufällig aus dem Replay Buffer. Dann wandeln wir die gesampleten Werte in np arrays um. Jetzt folgen wir dem Double Deep Q Learning Algorithmus und sagen Q Werte für die gesampleten nächsten Beobachtungen mit unserem Target Netz hervor. Mit Hilfe der Bellmann Gleichung erhalten wir so die Werte, die das Neuronale Netz lernen soll zu approximieren.\n",
    "\n",
    "Das eigentliche Training erfolgt dann mit der Standard Keras fit API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    obses = [t[0] for t in transitions]\n",
    "    for i, o in enumerate(obses):\n",
    "        if type(o) == tuple:\n",
    "            obses[i] = o[0]\n",
    "            o = o[0]\n",
    "\n",
    "    obses = np.stack(obses)\n",
    "    actions = np.array([t[1] for t in transitions])\n",
    "    rewards = np.array([t[2] for t in transitions])\n",
    "    next_obses = np.stack([t[3] for t in transitions])\n",
    "    dones = np.array([t[4] for t in transitions], dtype=np.int16)\n",
    "    \n",
    "\n",
    "    # TODO: Train Network based on Bellmann Equation\n",
    "    q_values = online_net(obses).numpy()\n",
    "    next_q_values = target_net(next_obses).numpy()\n",
    "\n",
    "    targets = []\n",
    "    for i, q in enumerate(q_values):\n",
    "        target = q\n",
    "        target[actions[i]] = rewards[i] + GAMMA * (1-dones[i]) * np.amax(next_q_values[i])\n",
    "        targets.append(target)\n",
    "    targets = np.stack(targets)\n",
    "    online_net.fit(obses, targets, epochs=1, batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Training starten wir einen potenziell endlosen Loop. Zu jedem Zeitschritt berechnen wir das aktuelle Epsilon basierend auf linearer Extrapolation. Dann wählen wir eine Aktion basierend auf unserer Epsilon-greedy Policy. Wir trainieren das online Modell alle `TRAINING_FREQ` Zeitschritte. Außerdem kopieren wir alle `TARGET_UPDATE_FREQ` Zeitschritte die Gewichte des online Netzes in das target Netz. Außerdem printen wir regelmäßig den mittleren Reward über die letzten 100 Zeitschritte, um den Trainingsfortschritt zu beurteilen. Falls wir einen vordefinierten maximalen Reward erreichen sagen wir das Training ist beendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m obs \u001b[39m=\u001b[39m step_env(action\u001b[39m=\u001b[39maction, last_obs\u001b[39m=\u001b[39mobs)\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m (step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m TRAINING_FREQ \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     training()\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m (step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m TARGET_UPDATE_FREQ \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m     target_net\u001b[39m.\u001b[39mset_weights(online_net\u001b[39m.\u001b[39mget_weights())\n",
      "Cell \u001b[1;32mIn[17], line 27\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     targets\u001b[39m.\u001b[39mappend(target)\n\u001b[0;32m     26\u001b[0m targets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(targets)\n\u001b[1;32m---> 27\u001b[0m online_net\u001b[39m.\u001b[39;49mfit(obses, targets, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    941\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 942\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[0;32m    943\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    945\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:763\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    761\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    762\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[39m.\u001b[39m_get_concrete_function_internal_garbage_collected(\n\u001b[0;32m    765\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[0;32m    768\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m--> 171\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:166\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[0;32m    164\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:396\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m   args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[0;32m    394\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[1;32m--> 396\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[0;32m    397\u001b[0m     args, kwargs, func_graph)\n\u001b[0;32m    399\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_function_captures  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:300\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[1;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[0;32m    299\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[1;32m--> 300\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m    301\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m    302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m    303\u001b[0m         args,\n\u001b[0;32m    304\u001b[0m         kwargs,\n\u001b[0;32m    305\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    306\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[0;32m    307\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m    308\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m    309\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m    310\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[0;32m    311\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m    313\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m    314\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    319\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1214\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1214\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1216\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:667\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    664\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    665\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 667\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39m__wrapped__(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    668\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1189\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[0;32m   1190\u001b[0m       original_func,\n\u001b[0;32m   1191\u001b[0m       args,\n\u001b[0;32m   1192\u001b[0m       kwargs,\n\u001b[0;32m   1193\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[0;32m   1194\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1195\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[0;32m   1196\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1197\u001b[0m       ))\n\u001b[0;32m   1198\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileuwonprb1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\engine\\training.py:1268\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1265\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m     )\n\u001b[0;32m   1267\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1269\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1270\u001b[0m     outputs,\n\u001b[0;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1272\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1273\u001b[0m )\n\u001b[0;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1311\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[0;32m   1312\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   1315\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 1316\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2895\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2893\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   2894\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 2895\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3696\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   3695\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m-> 3696\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\engine\\training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1249\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\engine\\training.py:1050\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m-> 1050\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1051\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\engine\\training.py:558\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    556\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 558\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1144\u001b[0m ):\n\u001b[1;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:427\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m   program_ctx \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39mProgramContext(options\u001b[39m=\u001b[39moptions)\n\u001b[1;32m--> 427\u001b[0m   converted_f \u001b[39m=\u001b[39m _convert_actual(target_entity, program_ctx)\n\u001b[0;32m    428\u001b[0m   \u001b[39mif\u001b[39;00m logging\u001b[39m.\u001b[39mhas_verbosity(\u001b[39m2\u001b[39m):\n\u001b[0;32m    429\u001b[0m     _log_callargs(converted_f, effective_args, kwargs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:269\u001b[0m, in \u001b[0;36m_convert_actual\u001b[1;34m(entity, program_ctx)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(entity, \u001b[39m'\u001b[39m\u001b[39m__code__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    265\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot apply autograph to a function that doesn\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    266\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mexpose a __code__ object. If this is a @tf.function,\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    267\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m try passing f.python_function instead.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m transformed, module, source_map \u001b[39m=\u001b[39m _TRANSPILER\u001b[39m.\u001b[39;49mtransform(entity, program_ctx)\n\u001b[0;32m    271\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformed, \u001b[39m'\u001b[39m\u001b[39mag_module\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    272\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformed, \u001b[39m'\u001b[39m\u001b[39mag_source_map\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py:282\u001b[0m, in \u001b[0;36mGenericTranspiler.transform\u001b[1;34m(self, obj, user_context)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transforms a Python object.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \n\u001b[0;32m    269\u001b[0m \u001b[39mUsers typically call this method.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39m  NotImplementedError: if the type of obj is not handled.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39misfunction(obj) \u001b[39mor\u001b[39;00m inspect\u001b[39m.\u001b[39mismethod(obj):\n\u001b[1;32m--> 282\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_function(obj, user_context)\n\u001b[0;32m    284\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mNon-function: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(obj)))\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py:466\u001b[0m, in \u001b[0;36mPyToPy.transform_function\u001b[1;34m(self, fn, user_context)\u001b[0m\n\u001b[0;32m    464\u001b[0m logging\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not cached for subkey \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, fn, cache_subkey)\n\u001b[0;32m    465\u001b[0m \u001b[39m# TODO(mdan): Confusing overloading pattern. Fix.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m nodes, ctx \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(PyToPy, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mtransform_function(fn, user_context)\n\u001b[0;32m    468\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(nodes, gast\u001b[39m.\u001b[39mLambda):\n\u001b[0;32m    469\u001b[0m   nodes \u001b[39m=\u001b[39m gast\u001b[39m.\u001b[39mAssign(\n\u001b[0;32m    470\u001b[0m       targets\u001b[39m=\u001b[39m[\n\u001b[0;32m    471\u001b[0m           gast\u001b[39m.\u001b[39mName(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    476\u001b[0m       ],\n\u001b[0;32m    477\u001b[0m       value\u001b[39m=\u001b[39mnodes)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py:359\u001b[0m, in \u001b[0;36mGenericTranspiler.transform_function\u001b[1;34m(self, fn, user_context)\u001b[0m\n\u001b[0;32m    356\u001b[0m context \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mContext(entity_info, namer, user_context)\n\u001b[0;32m    358\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_erase_arg_defaults(node)\n\u001b[1;32m--> 359\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_ast(node, context)\n\u001b[0;32m    361\u001b[0m \u001b[39mreturn\u001b[39;00m result, context\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:253\u001b[0m, in \u001b[0;36mPyToTF.transform_ast\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    251\u001b[0m   node \u001b[39m=\u001b[39m slices\u001b[39m.\u001b[39mtransform(node, ctx)\n\u001b[0;32m    252\u001b[0m node \u001b[39m=\u001b[39m call_trees\u001b[39m.\u001b[39mtransform(node, ctx)\n\u001b[1;32m--> 253\u001b[0m node \u001b[39m=\u001b[39m control_flow\u001b[39m.\u001b[39;49mtransform(node, ctx)\n\u001b[0;32m    254\u001b[0m node \u001b[39m=\u001b[39m conditional_expressions\u001b[39m.\u001b[39mtransform(node, ctx)\n\u001b[0;32m    255\u001b[0m node \u001b[39m=\u001b[39m logical_expressions\u001b[39m.\u001b[39mtransform(node, ctx)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\converters\\control_flow.py:408\u001b[0m, in \u001b[0;36mtransform\u001b[1;34m(node, ctx)\u001b[0m\n\u001b[0;32m    406\u001b[0m node \u001b[39m=\u001b[39m qual_names\u001b[39m.\u001b[39mresolve(node)\n\u001b[0;32m    407\u001b[0m node \u001b[39m=\u001b[39m activity\u001b[39m.\u001b[39mresolve(node, ctx, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 408\u001b[0m node \u001b[39m=\u001b[39m reaching_definitions\u001b[39m.\u001b[39;49mresolve(node, ctx, graphs)\n\u001b[0;32m    409\u001b[0m node \u001b[39m=\u001b[39m reaching_fndefs\u001b[39m.\u001b[39mresolve(node, ctx, graphs)\n\u001b[0;32m    410\u001b[0m node \u001b[39m=\u001b[39m liveness\u001b[39m.\u001b[39mresolve(node, ctx, graphs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:287\u001b[0m, in \u001b[0;36mresolve\u001b[1;34m(node, source_info, graphs, definition_factory)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resolves reaching definitions for each symbol.\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \n\u001b[0;32m    278\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39m  ast.AST\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m visitor \u001b[39m=\u001b[39m TreeAnnotator(source_info, graphs, definition_factory)\n\u001b[1;32m--> 287\u001b[0m node \u001b[39m=\u001b[39m visitor\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    288\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:269\u001b[0m, in \u001b[0;36mTreeAnnotator.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex):\n\u001b[0;32m    268\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex[node]\n\u001b[1;32m--> 269\u001b[0m node \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(TreeAnnotator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m parent\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m processing_expr_node:\n\u001b[0;32m    439\u001b[0m   entry_expr_value \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mvalue\n\u001b[1;32m--> 441\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Base, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    443\u001b[0m \u001b[39m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m (processing_expr_node \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(result, gast\u001b[39m.\u001b[39mExpr) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     (result\u001b[39m.\u001b[39mvalue \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m entry_expr_value)):\n\u001b[0;32m    447\u001b[0m   \u001b[39m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[0;32m    448\u001b[0m   \u001b[39m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[0;32m    449\u001b[0m   \u001b[39m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:195\u001b[0m, in \u001b[0;36mTreeAnnotator.visit_FunctionDef\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer \u001b[39m=\u001b[39m analyzer\n\u001b[0;32m    194\u001b[0m node\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(node\u001b[39m.\u001b[39margs)\n\u001b[1;32m--> 195\u001b[0m node\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit_block(node\u001b[39m.\u001b[39;49mbody)\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer \u001b[39m=\u001b[39m parent_analyzer\n\u001b[0;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py:336\u001b[0m, in \u001b[0;36mNodeStateTracker.visit_block\u001b[1;34m(self, nodes, before_visit, after_visit)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mif\u001b[39;00m before_visit:\n\u001b[0;32m    333\u001b[0m   \u001b[39m# TODO(mdan): We can modify node here too, if ever needed.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m   before_visit()\n\u001b[1;32m--> 336\u001b[0m replacement \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m after_visit \u001b[39mand\u001b[39;00m replacement:\n\u001b[0;32m    339\u001b[0m   replacement, new_destination \u001b[39m=\u001b[39m after_visit(replacement)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:269\u001b[0m, in \u001b[0;36mTreeAnnotator.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex):\n\u001b[0;32m    268\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex[node]\n\u001b[1;32m--> 269\u001b[0m node \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(TreeAnnotator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m parent\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m processing_expr_node:\n\u001b[0;32m    439\u001b[0m   entry_expr_value \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mvalue\n\u001b[1;32m--> 441\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Base, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    443\u001b[0m \u001b[39m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m (processing_expr_node \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(result, gast\u001b[39m.\u001b[39mExpr) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     (result\u001b[39m.\u001b[39mvalue \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m entry_expr_value)):\n\u001b[0;32m    447\u001b[0m   \u001b[39m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[0;32m    448\u001b[0m   \u001b[39m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[0;32m    449\u001b[0m   \u001b[39m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:494\u001b[0m, in \u001b[0;36mNodeTransformer.generic_visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m old_value:\n\u001b[0;32m    493\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, AST):\n\u001b[1;32m--> 494\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(value)\n\u001b[0;32m    495\u001b[0m         \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:269\u001b[0m, in \u001b[0;36mTreeAnnotator.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex):\n\u001b[0;32m    268\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex[node]\n\u001b[1;32m--> 269\u001b[0m node \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(TreeAnnotator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m parent\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m processing_expr_node:\n\u001b[0;32m    439\u001b[0m   entry_expr_value \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mvalue\n\u001b[1;32m--> 441\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Base, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    443\u001b[0m \u001b[39m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m (processing_expr_node \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(result, gast\u001b[39m.\u001b[39mExpr) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     (result\u001b[39m.\u001b[39mvalue \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m entry_expr_value)):\n\u001b[0;32m    447\u001b[0m   \u001b[39m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[0;32m    448\u001b[0m   \u001b[39m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[0;32m    449\u001b[0m   \u001b[39m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:503\u001b[0m, in \u001b[0;36mNodeTransformer.generic_visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    501\u001b[0m     old_value[:] \u001b[39m=\u001b[39m new_values\n\u001b[0;32m    502\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(old_value, AST):\n\u001b[1;32m--> 503\u001b[0m     new_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(old_value)\n\u001b[0;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m new_node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    505\u001b[0m         \u001b[39mdelattr\u001b[39m(node, field)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:269\u001b[0m, in \u001b[0;36mTreeAnnotator.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex):\n\u001b[0;32m    268\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex[node]\n\u001b[1;32m--> 269\u001b[0m node \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(TreeAnnotator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m parent\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m processing_expr_node:\n\u001b[0;32m    439\u001b[0m   entry_expr_value \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mvalue\n\u001b[1;32m--> 441\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Base, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    443\u001b[0m \u001b[39m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m (processing_expr_node \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(result, gast\u001b[39m.\u001b[39mExpr) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     (result\u001b[39m.\u001b[39mvalue \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m entry_expr_value)):\n\u001b[0;32m    447\u001b[0m   \u001b[39m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[0;32m    448\u001b[0m   \u001b[39m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[0;32m    449\u001b[0m   \u001b[39m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:503\u001b[0m, in \u001b[0;36mNodeTransformer.generic_visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    501\u001b[0m     old_value[:] \u001b[39m=\u001b[39m new_values\n\u001b[0;32m    502\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(old_value, AST):\n\u001b[1;32m--> 503\u001b[0m     new_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(old_value)\n\u001b[0;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m new_node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    505\u001b[0m         \u001b[39mdelattr\u001b[39m(node, field)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:269\u001b[0m, in \u001b[0;36mTreeAnnotator.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex):\n\u001b[0;32m    268\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex[node]\n\u001b[1;32m--> 269\u001b[0m node \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(TreeAnnotator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m parent\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m processing_expr_node:\n\u001b[0;32m    439\u001b[0m   entry_expr_value \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mvalue\n\u001b[1;32m--> 441\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Base, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    443\u001b[0m \u001b[39m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m (processing_expr_node \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(result, gast\u001b[39m.\u001b[39mExpr) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     (result\u001b[39m.\u001b[39mvalue \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m entry_expr_value)):\n\u001b[0;32m    447\u001b[0m   \u001b[39m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[0;32m    448\u001b[0m   \u001b[39m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[0;32m    449\u001b[0m   \u001b[39m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:503\u001b[0m, in \u001b[0;36mNodeTransformer.generic_visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    501\u001b[0m     old_value[:] \u001b[39m=\u001b[39m new_values\n\u001b[0;32m    502\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(old_value, AST):\n\u001b[1;32m--> 503\u001b[0m     new_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(old_value)\n\u001b[0;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m new_node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    505\u001b[0m         \u001b[39mdelattr\u001b[39m(node, field)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:269\u001b[0m, in \u001b[0;36mTreeAnnotator.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex):\n\u001b[0;32m    268\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_analyzer\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mindex[node]\n\u001b[1;32m--> 269\u001b[0m node \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(TreeAnnotator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_cfg_node \u001b[39m=\u001b[39m parent\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m processing_expr_node:\n\u001b[0;32m    439\u001b[0m   entry_expr_value \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mvalue\n\u001b[1;32m--> 441\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Base, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mvisit(node)\n\u001b[0;32m    443\u001b[0m \u001b[39m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m (processing_expr_node \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(result, gast\u001b[39m.\u001b[39mExpr) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     (result\u001b[39m.\u001b[39mvalue \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m entry_expr_value)):\n\u001b[0;32m    447\u001b[0m   \u001b[39m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[0;32m    448\u001b[0m   \u001b[39m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[0;32m    449\u001b[0m   \u001b[39m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\reaching_definitions.py:214\u001b[0m, in \u001b[0;36mTreeAnnotator.visit_Name\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    212\u001b[0m qn \u001b[39m=\u001b[39m anno\u001b[39m.\u001b[39mgetanno(node, anno\u001b[39m.\u001b[39mBasic\u001b[39m.\u001b[39mQN)\n\u001b[0;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(node\u001b[39m.\u001b[39mctx, gast\u001b[39m.\u001b[39mLoad):\n\u001b[1;32m--> 214\u001b[0m   anno\u001b[39m.\u001b[39;49msetanno(node, anno\u001b[39m.\u001b[39;49mStatic\u001b[39m.\u001b[39;49mDEFINITIONS,\n\u001b[0;32m    215\u001b[0m                \u001b[39mtuple\u001b[39;49m(analyzer\u001b[39m.\u001b[39;49min_[cfg_node]\u001b[39m.\u001b[39;49mvalue\u001b[39m.\u001b[39;49mget(qn, ())))\n\u001b[0;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m   anno\u001b[39m.\u001b[39msetanno(node, anno\u001b[39m.\u001b[39mStatic\u001b[39m.\u001b[39mDEFINITIONS,\n\u001b[0;32m    218\u001b[0m                \u001b[39mtuple\u001b[39m(analyzer\u001b[39m.\u001b[39mout[cfg_node]\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mget(qn, ())))\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\anno.py:140\u001b[0m, in \u001b[0;36msetanno\u001b[1;34m(node, key, value, field_name)\u001b[0m\n\u001b[0;32m    137\u001b[0m annotations[key] \u001b[39m=\u001b[39m value\n\u001b[0;32m    139\u001b[0m \u001b[39m# So that the annotations survive gast_to_ast() and ast_to_gast()\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[39mif\u001b[39;00m field_name \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m node\u001b[39m.\u001b[39;49m_fields:\n\u001b[0;32m    141\u001b[0m   node\u001b[39m.\u001b[39m_fields \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (field_name,)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "obs = env.reset()\n",
    "for step in itertools.count():\n",
    "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
    "    action = get_epsilon_greedy_action(obs, epsilon)\n",
    "    obs = step_env(action=action, last_obs=obs)\n",
    "\n",
    "    if (step+1) % TRAINING_FREQ == 0:\n",
    "        training()\n",
    "\n",
    "    if (step+1) % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.set_weights(online_net.get_weights())\n",
    "\n",
    "    if (step+1) % 500 == 0:\n",
    "        print()\n",
    "        print(f'Step: {step+1}')\n",
    "        print(f'Avg Reward: {np.mean(reward_buffer)}')\n",
    "    \n",
    "    if len(reward_buffer) >= 100:\n",
    "        if np.mean(reward_buffer) >= 195:\n",
    "            print(\"Solved Problem\")\n",
    "            break\n",
    "    \n",
    "    if step == 40000:\n",
    "      break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie verhält sich unser trainierter Agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "restored_net = keras.models.load_model(\"my_cartpole_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m obs, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(restored_net\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49mexpand_dims(obs, \u001b[39m0\u001b[39;49m), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n\u001b[0;32m      8\u001b[0m     obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\keras\\engine\\training.py:2382\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2380\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2381\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2382\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2383\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2384\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    931\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    935\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\Ingenieruswissenschaftliches Kolleg\\QandA\\Reinforcement Learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    action = np.argmax(restored_net.predict(np.expand_dims(obs, 0), verbose=0))\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
